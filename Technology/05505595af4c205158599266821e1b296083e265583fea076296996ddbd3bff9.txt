old tech dies, usually stays dead. one expects rotary phones adding machines come crawling back oblivion. Floppy diskettes, VHS tapes, cathode-ray tubes—they shall rest peace. Likewise, won’t see old analog computers data centers anytime soon. monstrous beasts: difficult program, expensive maintain, limited accuracy.
thought. came across confounding statement:
Bringing back analog computers much advanced forms historic ancestors change world computing drastically forever.
Seriously?
found prediction preface handsome illustrated book titled, simply, Analog Computing. Reissued 2022, written German mathematician Bernd Ulmann—who seemed serious indeed.
article appears May 2023 issue. Subscribe WIRED.Illustration: Alvaro Dominguez
I’ve writing future tech since WIRED existed written six books explaining electronics. used develop software, friends design hardware. I’d never heard anyone say anything analog, would Ulmann imagine dead paradigm could resurrected? far-reaching permanent consequences?
felt compelled investigate further.
example digital displaced analog, look photography. pre-digital camera, continuous variations light created chemical reactions piece film, image appeared representation—an analogue—of reality. modern camera, contrast, light variations converted digital values. processed camera’s CPU saved stream 1s 0s—with digital compression, wish.
Engineers began using word analog 1940s (shortened analogue; like compression) refer computers simulated real-world conditions. mechanical devices much thing centuries.
Antikythera mechanism astonishingly complex piece machinery used thousands years ago ancient Greece. Containing least 30 bronze gears, displayed everyday movements moon, sun, five planets also predicting solar lunar eclipses. mechanical workings simulated real-world celestial events, regarded one earliest analog computers.
centuries passed, mechanical analog devices fabricated earthlier purposes. 1800s, invention called planimeter consisted little wheel, shaft, linkage. traced pointer around edge shape piece paper, area shape displayed scale. tool became indispensable item real-estate offices buyers wanted know acreage irregularly shaped piece land.
gadgets served military needs. battleship trying aim 16-inch gun target beyond horizon, needed assess orientation ship, motion, position, direction speed wind; clever mechanical components allowed operator input factors adjust gun appropriately. Gears, linkages, pulleys, levers could also predict tides calculate distances map.
1940s, electronic components vacuum tubes resistors added, fluctuating current flowing could analogous behavior fluids, gases, phenomena physical world. varying voltage could represent velocity Nazi V2 missile fired London, example, orientation Gemini space capsule 1963 flight simulator.
then, analog become dying art. Instead using voltage represent velocity missile electrical resistance represent air resistance slowing down, digital computer could convert variables binary code—streams 1s 0s suitable processing. Early digital computers massive mainframes full vacuum tubes, integrated circuit chips made digital processing cheaper, reliable, versatile. 1970s, analog-digital difference could summarized like this:
last factor big deal, accuracy analog computers always limited components. Whether used gear wheels vacuum tubes chemical film, precision limited manufacturing tolerances deteriorated age. Analog always modeled real world, world never absolutely precise.
nerdy British schoolboy mild case OCD, inaccuracy bothered lot. revered Pythagoras, told triangle sides 3 centimeters 4 centimeters adjacent 90-degree angle would diagonal side 5 centimeters, precisely. Alas, pleasure diminished realized proof applied theoretical realm lines zero thickness.
everyday realm, precision limited ability sharpen pencil, tried make measurements, ran another bothersome feature reality. Using magnifying glass, compared ruler I’d bought stationery store ruler school’s physics lab, discovered exactly length.
could be? Seeking enlightenment, checked history metric system. meter fundamental unit, birthed bizarre combination nationalism whimsy. French Revolution, new government instituted meter get away imprecision ancien régime. French Academy Sciences defined longitudinal distance equator, Paris, North Pole, divided 10 million. 1799, meter solemnified like religious totem form platinum bar French National Archives. Copies made distributed across Europe Americas, copies made copies’ copies. process introduced transcription errors, eventually led traumatic discovery rulers different sources might visibly unequal.
Similar problems impeded definitive measurement time, temperature, mass. conclusion inescapable adolescent mind: hoping absolute precision physical realm, couldn’t it.
personal term inexact nature messy, fuzzy world muzzy. then, 1980, acquired Ohio Scientific desktop computer found prompt, lasting relief. operations built foundation binary arithmetic, 1 always exactly 1 0 genuine 0, fractional quibbling. 1 existence, 0 nothingness! fell love purity digital learned write code, became lifelong refuge muzzy math.
course, digital values still stored fallible physical components, margins error took care that. modern 5-volt digital chip, 1.5 volts lower would represent number 0 3.5 volts greater would represent number 1. Components decently engineered motherboard would stay within limits, shouldn’t misunderstandings.
Consequently, Bernd Ulmann predicted analog computers due zombie comeback, wasn’t skeptical. found idea bit … disturbing.
Hoping reality check, consulted Lyle Bickley, founding member Computer History Museum Mountain View, California. served years expert witness patent suits, Bickley maintains encyclopedic knowledge everything done still done data processing.
Bickley explained when, say, brute-force natural-language AI systems distill millions words internet, process insanely power hungry. human brain runs small amount electricity, said, 20 watts. (That’s light bulb.) “Yet try thing digital computers, takes megawatts.” kind application, digital “not going work. It’s smart way it.”
Bickley said would violating confidentiality tell specifics, went looking startups. Quickly found San Francisco Bay Area company called Mythic, claimed marketing “industry-first AI analog matrix processor.”
Mike Henry cofounded Mythic University Michigan 2013. He’s energetic guy neat haircut well-ironed shirt, like old-time IBM salesman. expanded Bickley’s point, citing brain-like neural network powers GPT-3. “It 175 billion synapses,” Henry said, comparing processing elements connections neurons brain. “So every time run model one thing, load 175 billion values. large data-center systems barely keep up.”
That’s because, Henry said, digital. Modern AI systems use type memory called static RAM, SRAM, requires constant power store data. circuitry must remain switched even it’s performing task. Engineers done lot improve efficiency SRAM, there’s limit. “Tricks like lowering supply voltage running out,” Henry said.
Mythic’s analog chip uses less power storing neural weights SRAM flash memory, doesn’t consume power retain state. flash memory embedded processing chip, configuration Mythic calls “compute-in-memory.” Instead consuming lot power moving millions bytes back forth memory CPU (as digital computer does), processing done locally.
bothered Mythic seemed reintroducing accuracy problems analog. flash memory storing 1 0 comfortable margins error, like old-school logic chips. holding intermediate voltages (as many 256 them!) simulate varying states neurons brain, wonder whether voltages would drift time. Henry didn’t seem think would.
another problem chip: way worked hard explain. Henry laughed. “Welcome life,” said. “Try explaining venture capitalists.” Mythic’s success front variable: Shortly spoke Henry, company ran cash. (More recently raised $13 million new funding appointed new CEO.)
next went IBM. corporate PR department connected Vijay Narayanan, researcher company’s physics-of-AI department. preferred interact via company-sanctioned email statements.
moment, Narayanan wrote, “our analog research customizing AI hardware, particularly energy efficiency.” So, goal Mythic. However, Narayanan seemed rather circumspect details, reading found IBM paper referred “no appreciable accuracy loss” memory systems. appreciable loss? mean loss? durability issue. Another paper mentioned “an accuracy 93.5 percent retained one-day period.” lost 6.5 percent one day? bad? compared to?
many unanswered questions, biggest letdown this: Mythic IBM seemed interested analog computing insofar specific analog processes could reduce energy storage requirements AI—not perform fundamental bit-based calculations. (The digital components would still that.) far could tell, wasn’t anything close second coming analog predicted Ulmann. computers yesteryear may room-sized behemoths, could simulate everything liquid flowing pipe nuclear reactions. applications shared one attribute. dynamic. involved concept change.
Engineers began using word analog 1940s refer computers simulated real-world conditions.
Another childhood conundrum: held ball dropped it, force gravity made move increasing speed. could figure total distance ball traveled speed changing continuously time? could break journey seconds milliseconds microseconds, work speed step, add distances. time actually flowed tiny steps, speed would jump instantaneously one step next. could true?
Later learned questions addressed Isaac Newton Gottfried Leibniz centuries ago. They’d said velocity change increments, increments infinitely small.
steps, weren’t really steps? sounded like evasion me, iffy premise, Newton Leibniz developed calculus, enabling everyone calculate behavior countless naturally changing aspects world. Calculus way mathematically modeling something that’s continuously changing, like distance traversed falling ball, sequence infinitely small differences: differential equation.
math could used input old-school analog electronic computers—often called, reason, differential analyzers. could plug components together represent operations equation, set values using potentiometers, answer could shown almost immediately trace oscilloscope screen. might ideally accurate, muzzy world, learned discontent, nothing ideally accurate.
competitive, true analog computer could emulate versatile behavior would suitable low-cost mass production—on scale silicon chip. thing developed? went back Ulmann’s book found answer penultimate page. researcher named Glenn Cowan created genuine VLSI (very large-scale integrated circuit) analog chip back 2003. Ulmann complained “limited capabilities,” sounded like real deal.
Glenn Cowan studious, methodical, amiable man professor electrical engineering Montreal’s Concordia University. grad student Columbia back 1999, choice two research topics: One would entail optimizing single transistor, would develop entirely new analog computer. latter pet project adviser named Yannis Tsividis. “Yannis sort convinced me,” Cowan told me, sounding wasn’t quite sure happened.
Initially, specifications, one ever built analog computer chip. Cowan didn’t know accurate could basically making went along. take courses Columbia fill gaps knowledge. Two years later, test chip that, told modestly, “full graduate-student naivete. looked like breadboarding nightmare.” Still, worked, decided stick around make better version. took another two years.
key innovation Cowan’s making chip reconfigurable—or programmable. Old-school analog computers used clunky patch cords plug boards. Cowan thing miniature, areas chip itself, using preexisting technology known transmission gates. work solid-state switches connect output processing block input block B, block C, block choose.
second innovation make analog chip compatible off-the-shelf digital computer, could help circumvent limits precision. “You could get approximate analog solution starting point,” Cowan explained, “and feed digital computer guess, iterative routines converge faster good guess.” end result great labor etched onto silicon wafer measuring respectable 10 millimeters 10 millimeters. “Remarkably,” told me, “it work.”
asked Cowan real-world uses, inevitably mentioned AI. I’d time think neural nets beginning feel skeptical. standard neural net setup, known crossbar configuration, cell net connects four cells. may layered allow extra connections, even so, they’re far less complex frontal cortex brain, individual neuron connected 10,000 others. Moreover, brain static network. first year life, new neural connections form rate 1 million per second. saw way neural network emulate processes like that.
Glenn Cowan’s second analog chip wasn’t end story Columbia. Additional refinements necessary, Yannis Tsividis wait another graduate student would continue work.
2011 soft-spoken young man named Ning Guo turned willing. Like Cowan, never designed chip before. “I found it, um, pretty challenging,” told me. laughed memory shook head. “We optimistic,” recalled ruefully. laughed again. “Like thought could get done summer.”
fact, took year complete chip design. Guo said Tsividis required “90 percent confidence level” chip would work would proceed expensive process fabrication. Guo took chance, result named HCDC, meaning hybrid continuous discrete computer. Guo’s prototype incorporated board could interface off-the-shelf digital computer. outside, looked like accessory circuit board PC.
asked Guo possible applications, think bit. Instead mentioning AI, suggested tasks simulating lot moving mechanical joints would rigidly connected robotics. Then, unlike many engineers, allowed speculate.
diminishing returns digital model, said, yet still dominates industry. “If applied many people much money analog domain, think could kind analog coprocessing happening accelerate existing algorithms. Digital computers good scalability. Analog good complex interactions variables. future, may combine advantages.”
HCDC fully functional, problem: easy use. Fortuitously, talented programmer MIT named Sara Achour read project saw ideal target skills. specialist compilers—programs convert high-level programming language machine language—and could add user-friendly front end Python help people program chip. reached Tsividis, sent one precious boards fabricated.
spoke Achour, entertaining engaging, delivering terminology manic pace. told originally intended doctor switched computer science pursued programming hobby since middle school. “I specialized math modeling biological systems,” said. “We macroscopic modeling gene protein hormonal dynamics.” Seeing blank look, added: “We trying predict things like hormonal changes inject someone particular drug.”
Changes key word. fully acquainted math describe change, two years finished compiler analog chip. “I didn’t build, like, entry-level product,” said. “But made easier find resilient implementations computation want run. see, even people design type hardware difficulty programming it. It’s still extremely painful.”
liked idea former medical student alleviating pain chip designers difficulty using hardware. take applications? any?
“Yes, whenever you’re sensing environment,” said. “And reconfigurability lets reuse piece hardware multiple computations. don’t think going relegated niche model. Analog computation makes lot sense you’re interfacing something inherently analog.” Like real world, muzziness.
Going back concept dropping ball, interest finding far travels period time: Calculus solves problem easily, differential equation—if ignore air resistance. proper term “integrating velocity respect time.”
don’t ignore air resistance? faster ball falls, air resistance encounters. gravity remains constant, ball’s speed doesn’t increase steady rate tails reaches terminal velocity. express differential equation too, adds another layer complexity. won’t get mathematical notation (I prefer avoid pain it, use Sara Achour’s memorable term), take-home message matters. Every time introduce another factor, scenario gets complicated. there’s crosswind, ball collides balls, falls hole center Earth, gravity zero—the situation get discouragingly complicated.
suppose want simulate scenario using digital computer. It’ll need lot data points generate smooth curve, it’ll continually recalculate values point. calculations add up, especially multiple objects become involved. billions objects—as nuclear chain reaction, synapse states AI engine—you’ll need digital processor containing maybe 100 billion transistors crunch data billions cycles per second. cycle, switching operation transistor generate heat. Waste heat becomes serious issue.
Using new-age analog chip, express factors differential equation type Achour’s compiler, converts equation machine language chip understands. brute force binary code minimized, power consumption heat. HCDC like efficient little helper residing secretly amid modern hardware, it’s chip-sized, unlike room-sized behemoths yesteryear.
update basic analog attributes:
see designs Tsividis grad students addressed historic disadvantages previous list. yet, despite this, Tsividis—the prophet modern analog computing—still difficulty getting people take seriously.
Born Greece 1946, Tsividis developed early dislike geography, history, chemistry. “I felt facts memorize synapses brain,” told me. loved math physics ran different problem teacher assured perimeter circle three times diameter plus 14 centimeters. course, (approximately) 3.14 times diameter circle, Tsividis said so, teacher told quiet. This, said, “suggested rather strongly authority figures always right.”
taught English, started learning electronics, designed built devices like radio transmitters, eventually fled Greek college system compelled learn organic chemistry. 1972 began graduate studies United States, years became known challenging orthodoxy field computer science. One well-known circuit designer referred “the analog MOS freak,” designed fabricated amplifier chip 1975 using metal-oxide semiconductor technology, absolutely one believed suitable task.
days, Tsividis polite earth, interest wasting words. attempt bring back analog form integrated chips began earnest late ’90s. talked him, told 18 boards analog chips mounted them, couple loaned researchers Achour. “But project hold now,” said, “because funding ended National Science Foundation. two years Covid.”
“I would need know, put together many chips model large system, happens? try put together many chips eventually, help silicon foundries, make large computer single chip.”
pointed development far already taken almost 20 years.
“Yes, several years breaks between. Whenever appropriate funding, revive process.”
asked whether state analog computing today could compared quantum computing 25 years ago. Could follow similar path development, fringe consideration common (and well-funded) acceptance?
would take fraction time, said. “We experimental results. proven itself. group wants make user-friendly, within year could it.” point willing provide analog computer boards interested researchers, use Achour’s compiler.
sort people would qualify?
“The background need computers. really need math background know differential equations are.”
asked whether felt idea was, way, obvious. hadn’t resonated yet people?
“People wonder everything digital. say digital future, digital future—and course it’s future. physical world analog, big interface. That’s fits.”
digital processor crunching data billions cycles per second, switching operation transistor generates heat.
Illustration: Khyati Trehan
Tsividis mentioned offhandedly people applying analog computation would need appropriate math background, started wonder. Developing algorithms digital computers strenuous mental exercise, calculus seldom required. mentioned Achour, laughed said submits papers reviewers, “Some say haven’t seen differential equations years. never seen differential equations.”
doubt lot won’t want to. financial incentives way overcoming resistance change. Imagine future software engineers command extra $100K per annum adding new bullet point résumé: “Fluent differential equations.” happens, I’m thinking Python developers soon signing remedial online calculus classes.
Likewise, business, determining factor financial. There’s going lot money AI—and smarter drug molecules, agile robots, dozen applications model muzzy complexity physical world. power consumption heat dissipation become really expensive problems, shunting digital load miniaturized analog coprocessors significantly cheaper, one care analog computation used done math-genius grandfather using big steel box full vacuum tubes.
Reality really imprecise, matter much would prefer otherwise, want model truly exquisite fidelity, digitizing may sensible method. Therefore, must conclude:
idealistic community exchanging free stuff tried break away Facebook, ended breaking apart.
Vauhini Vara
WIRED tomorrow realized. essential source information ideas make sense world constant transformation. WIRED conversation illuminates technology changing every aspect lives—from culture business, science design. breakthroughs innovations uncover lead new ways thinking, new connections, new industries.
