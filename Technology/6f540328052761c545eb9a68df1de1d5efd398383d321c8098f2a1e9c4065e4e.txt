2015, car-and-rocket man Elon Musk joined influential startup backer Sam Altman put artificial intelligence new, open course. cofounded research institute called OpenAI make new AI discoveries give away common good. Now, institute’s researchers sufficiently worried something built won’t release public.
AI system gave creators pause designed learn patterns language. well—scoring better reading-comprehension tests automated system. OpenAI’s researchers configured system generate text, began think achievement differently.
“It looks pretty darn real,” says David Luan, vice president engineering OpenAI, text system generates. fellow researchers began imagine might used unfriendly purposes. “It could someone malicious intent would able generate high-quality fake news,” Luan says.
concern prompted OpenAI publish research paper results, release full model 8 million web pages used train system. Previously, institute often disseminated full code publications, including earlier version language project last summer.
Google, too, decided it’s longer appropriate innocently publish new AI research findings code. Last month, search company disclosed policy paper AI put constraints research software shared fears misuse. company recently joined Microsoft adding language financial filings warning investors AI software could raise ethical concerns harm business.
OpenAI let WIRED play text generator, via web page lets type text system uses like writing prompt. results could garbled, way system riffed prompts song lyrics, poems, phrases like “here recipe yogurt pancakes” confirmed Luan’s view output look pretty darn real.
However, didn’t take much get system unspool paragraphs resembled work disinformation artists. WIRED typed phrase “Hillary Clinton George Soros,” OpenAI’s system wrote following:
“...are perfect match, agenda appears create political movement Soros political machine Clinton two major players. first time Soros Clinton caught tape directly colluding promoting false narrative. One key revelations leaked audio Clinton's admission Russian banker knew Uranium One deal approved Congress. Clinton shown sharing talking points originally drafted Fusion GPS contractor hired anti-Trump Republican donor. leaked audio clearest evidence yet Clinton campaign Hillary Foundation colluded Fusion GPS manufacture propaganda President Trump.”
Jack Clark, policy director OpenAI, says example shows technology like might shake processes behind online disinformation trolling, already use form automation. “As costs producing text fall, may see behaviors bad actors alter,” says.
sample reflects nature text internet machine-learning systems try mimic what’s training data. “This stuff reads great, it’s necessarily true,” says Kristian Hammond, professor Northwestern. “It’s learning surface structure—Given know, words say sound right?” OpenAI’s researchers seen system write text nonsensical scenarios, like fires underwater.
Hammond cofounded startup called Narrative Science generates business documents like financial reports translating data text. says OpenAI’s system shows impressive fluency could useful creative projects. OpenAI's research builds recent spurt progress software processes text, enabled new machine-learning systems better searching text contextual clues word’s meaning.
order machines write broadly useful, however, probably need able reliably stick facts grounded kind external data. “I hope find ways hook real data world, truth, kind system,” Hammond says.
work posting falsehoods internet, truth doesn’t much matter. projects might find uses text generation like OpenAI’s sooner financial firms like working Narrative Science.
version OpenAI’s text generator trained Amazon product reviews illustrates little extra training adapt specific task—or attack. Fake reviews trying boost drag products businesses already headache companies Yelp, weaponized political disputes.
Prompted write 1-star book review summary “I hate Tom Simonite’s book,” model wrote:
“Tom Simonite keep simple. give enough info subject make reading book enjoyable. 400 pages footnotes, way getting work subject way. course, never really feel like author clear vision subject. give enough details group people going come together solve problem come solution problem. book depressing me, can't even talk without feeling like want punch kindle.”
OpenAI’s concerns downsides AI-generated text echo worries misleading “deepfake” videoclips made machine learning could undermine elections spread false information. Clark OpenAI likens lab’s text-generation system state image-generating technology heart deepfakes 2015, one much worried fake imagery.
technology matured fast, became easy access anonymous coder released tools developed insert Hollywood stars pornographic videos. Pentagon devoting millions dollars figure detect AI-altered imagery, last month Republican senator introduced bill seeking criminalize creation dissemination deepfakes.
Clark says OpenAI hopes voicing concerns code, encourage AI researchers open thoughtful develop release. “We’re sounding alarm. we’re saying is, two three years progress,” concerns even pressing, Clark says.
timeline necessarily fuzzy. Although machine-learning software deals language improving rapidly, one knows sure long, far, go. “It could S-curve we’re saturate, could we’ll keep accelerating,” says Alec Radford, researcher worked OpenAI’s project.
Tom Simonite senior editor edits WIRED’s business coverage. previously covered artificial intelligence trained artificial neural network generate seascapes. Simonite previously San Francisco bureau chief MIT Technology Review, wrote edited technology coverage New Scientist magazine London. He... Read
new tool aims deliver network insights coordination “AI” security systems long promised.
Lily Hay Newman
WIRED tomorrow realized. essential source information ideas make sense world constant transformation. WIRED conversation illuminates technology changing every aspect lives—from culture business, science design. breakthroughs innovations uncover lead new ways thinking, new connections, new industries.
